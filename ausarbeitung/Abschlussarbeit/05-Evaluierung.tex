\chapter{Evaluierung}

\section{Benutzerevaluierung}
Die Idee:
\begin{itemize}
\item Man baut eine einfache Webschnitstelle zum unserem System, wo der Benutzer Suchanfragen angeben kann.
\item Für jeden gefundenen Sniplet wird der Text des Sniplets und Liste von Ontologien (wie im Webinterface von Stanbol) geliefert.
\item Der Benutzer kann jeden verfügbaren Engine(TIGER/Stanford/Wiki) bewerten (ob der Engine hilfreich war, oder nicht, mit einer Note von 1 bis 10)
\item Als Erweiterung:
\begin{enumerate}
\item Gebe dem Benutzer den Auswahlmöglichkeit zwischen 'nur Sniplets untersuchen' und 'volle Webseiten untersuchen'.
\item Neben der Bewertung 'Ein Engine - eine Note' die Bewertung 'Ein Engine + Snipletsuntersuchung/Webseiteuntersuchung' einzuführen.
\end{enumerate}
\end{itemize}

\section{Automatische Evaluierung}{
\begin{itemize}
\item TIGER Korpus (oder Wiki?) als Evaluierungskorpus nehmen.
\item Recall/Precision/F-Measure berechnen
\item Gucken, ob es Korrelationen zwischen der Benutzer- und Autoevaluierung gibt - ob man anhand der o.g. Bewertungen (Precision/Recall) sagen kann, ob der Engine für den Benutzer hilfreich sein kann, oder nicht?
\end{itemize}

\section{Zusammenfassung}
Erkenntnissen aus der Studie und den Konsequenzen für die Arbeit.