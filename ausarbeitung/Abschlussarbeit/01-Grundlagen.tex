\chapter{Grundlagen}

\section{Extraktion von Entitäten} \label{sec:Grundlagen}
\paragraph{}
%Was sind die Entitäten - kurze Einleitung. Wie genau können die Entitäten aus einem Text extrahiert werden? Welche Einsätze gibt es? Wofür kann man Entitäten verwenden?
Vijay Krishnan hat in seiner Arbeit\cite{Vijay/Vignesh:05} die Extraktion von Entitäten als Suche nach atomaren Elementen im Text und ihre Zuordnung bestimmten vordefinierten Klassen wie Person, Organisation, geographische Lokation usw. definiert. Zum Beispiel betrachten wir folgenden Text aus Wikipedia: ,,Seit dem 1. Januar 2014 ist Bill de Blasio neuer Bürgermeister von New York.``. Dabei soll das Framework, das die Entitäten aus dem Text extrahiert, die Entität \textit{Bill de Blasio} als eine Person erkennen, die Entität \textit{New York} als ein geographisches Objekt, und \textit{1. Januar 2014} als ein Datum.

\paragraph{}
Es stellt sich die Frage, wie genau die Entitäten für die Benutzerunterstützung bei der Suche eingesetzt werden können. Wenn die einzige Informationen, die dem Benutzer zur Verfügung stehen würde, nur der Name, die Klasse und Position der Entität innerhalb des Textes wären, wären Entitäten kaum verwendbar. Aber wenn jeder Entität eine Menge von Eigenschaften (wie Geburtsdatum für eine Person) und Verbindungen zu anderen Entitäten (Z.b. Geburtsort einer Person könnte ein Link auf eine geografische Entität sein) zugeordnet wird, könnte der Benutzer theoretisch die aus der Entität gewonnene Informationen für Präzisierung der Suchanfrage verwenden.

\paragraph{}
Aber wie könnten die Entitäten aus einem natürlichsprachlichen Text extrahiert werden? Die einfachste Möglichkeit wäre der Einsatz von ,,fest definierten`` Regeln, die dann auf den ganzen Text angewendet werden, und entscheiden, ob das nächste Wort eine Entität ist oder nicht. Dieser Ansatz soll in dieser Arbeit aber nicht eingesetzt werden, da er folgende Nachteile\cite{baluja2000applying} hat:
\begin{enumerate}
\item So ein ,,festes`` System ist nicht in der Lage, Änderungen automatisch zu 'erlernen' und muss für jede Änderung neu angepasst/programmiert werden.
\item Derartige ,,feste`` Systeme neigen dazu, mit der Zeit sehr komplex zu werden, und ab irgendeinem Zeitpunkt wird die Anpassung von Regeln unmöglich beziehungsweise mit einem großen technischem Aufwand verbunden sein.
\item Solche Systeme können nicht wirklich gut mit den fehlerhaften Daten wie Schreibfehler arbeiten, was im Fall von Websuche sehr oft auftreten kann.
\end{enumerate}  
Der einzige Vorteil von solchen festen Systemen ist dass die für sehr kleine Domäne eventuell gut funktionieren können, was allerdings für Websuche nicht ausreichend ist.

Aber welche Methode ist für die Extraktion von Entitäten passend? Wie erwähnt, gehört jede Entität irgendeiner Klasse. Deswegen kann man die Extraktion von Entitäten als eine Klassifikationsaufgabe definieren, und die Methoden des maschinellen Lernens, die man für die Klassifizierung anwendet, einsetzen, wie es z. B. in der Arbeit von William Cohen\cite{cohen2004exploiting} beschrieben wurde. In dieser Arbeit werden drei verschiedene Einsätze verglichen und eingesetzt: \textbf{Conditional Random Field}, \textbf{Support Vector Machines} und \textbf{Maximum Entropy}.

\subsection{Features}
\paragraph{}
Wie für jede andere Klassifizierungsaufgabe müssen für die Extraktion von Entitäten die Parameter definiert werden, die jedes zu klassifizierendes Objekt beschreiben. Im Fall von Extraktion von Entitäten  heißt das, dass jedem Wort ein Vektor von Eigenschaften zugeordnet werden muss. Nadeau\cite{nadeau2007survey} hat die Eigenschaften, die ein Wort haben kann, in drei Gruppen geteilt:

\begin{itemize}
\item Boolesche Eigenschaften wie ,,Ob das Wort groß geschrieben wird``.
\item Numerische Eigenschaften wie die Länge des Wortes.
\item Nominale Eigenschaften wie Präfix oder Suffix.
\end{itemize}
Zusätzlich dazu wurde von Hai\cite{chieu2002named} die Verteilung in lokale und globale Eigenschaften vorgeschlagen:

\begin{enumerate}
\item Globale Eigenschaften:
\begin{enumerate}
\item Personenpräfixe für bestimmtes Wort in anderen Sätzen des Dokumentes: z. B. wenn wir im Text zuerst die Tokens ,,Frau Sony`` treffen, und dann einfach ,,Sony``, dann soll angenommen werden, dass ,,Sony`` eine Entität der Klasse ,,Person`` ist.
\item Abkürzungen: wenn in einem Satz mehr Wörter nacheinander groß geschrieben werden, wie z.B. ,,Deutsche Demokratische Republik``, dann wird in dem Text nach entsprechender Abkürzung gesucht: ,,DDR``, und wenn Abkürzung eine Entität ist, können auch alle entsprechende Tokens als eine Entität markiert werden.
\end{enumerate}
\item Lokale Eigenschaften:
\begin{enumerate}
\item Ob das Wort großgeschrieben wird.
\item Ob vorheriges oder nächstes Wort großgeschrieben wird.
\item Ob alle Zeichen im Wort großgeschrieben werden.
\item Ob es ein Punkt am Ende des Wortes steht.
\item Ob das Wort Zahlen beinhaltet, und falls ja, wie viel.
\item Ob das Wort das Prozent- oder Dollarzeichen beinhaltet.
\item Bestimmte Hilfswörter (wie ,,Frau`` oder ,,GmbH``), die entweder vor oder nach dem Wort, das untersucht werden muss, stehen.
\item Ob das Wort nur aus Zahlen besteht.
\item POS\footnote{Part Of Speech - Wortart}-Tag des Wortes (ob das Wort ein Substantiv oder ein Verb ist).
\item POS-Tags der Wörter, die vor dem ausgewählten Wort stehen.
\end{enumerate}
\end{enumerate}
Dabei beschreiben lokale Features die Eigenschaften, die nur die Informationen, die aus dem Wort selbst und aus den benachbarten Wörtern in demselben Satz gewonnen werden können, brauchen. Globale Eigenschaften benutzen dagegen auch die Vorkommnisse vom Wort im ganzen Dokument.

Das oben genannte Features werden ,,supervised Features`` genannt, weil die von einem Mensch bestimmt wurden. Zusätzlich dazu wird in der Arbeit von Turian\cite{turian2010word} der Einsatz von sogenannten ,,unsupervised Features`` vorgeschlagen, die während des Lernvorganges berechnet werden. Es wird in der zitierten Arbeit zwischen folgenden Gruppen von ,,unsupervised Features`` unterschieden:
\begin{enumerate}
\item Distributional Features - eine Matrix der Größe $W \times C$, wo $W$ die Anzahl von Wörtern im Wörterbuch beschreibt und $C$ die Anzahl von verschiedenen Kontexten.
\item Auf Cluster basierte Features - es wird versucht, die Wörter automatisch in Clusters zu teilen.
\item Word Embedding - Dieses Modell wird hauptsächlich in den neuronalen Modellen eingesetzt, und erlernt Eigenschaften aus N-Grammen, mithilfe von Lookup-Tabellen.
\end{enumerate}

\paragraph{}
Der Vorgang zum Aufbau eines ,,Distributional Features``-Modells wurde in der Arbeit von Sahlgren\cite{sahlgren2006word} beschrieben. Als Basisdatenstruktur wird die Co-occurence-Matrix verwendet, die beschreibt, welche Wörter zusammen in einem Kontext gesehen werden. Die Anzahl von Zeilen und Spalten in dieser Matrix ist dabei der Anzahl von Wörtern im zu analysierenden Korpus gleich. Ein Beispiel solcher Matrix für den Satz ,,Leela mag dramatische Filme und Bücher`` findet man in der Abbildung \ref{fig:COOC-MAT}.

\begin{figure}[ht]
\setbox0\vbox{\small}
$$
\begin{array}{ccccccc}
 & Leela & mag & dramatische & Filme & und & B\ddot{u}cher \\ 
Leela & 0 & 1 & 0 & 0 & 0 & 0 \\ 
mag & 1 & 0 & 1 & 0 & 0 & 0 \\ 
dramatische & 0 & 1 & 0 & 1 & 0 & 0 \\ 
Filme & 0 & 0 & 1 & 0 & 1 & 0 \\ 
und & 0 & 0 & 0 & 1 & 0 & 1 \\ 
B\ddot{u}cher & 0 & 0 & 0 & 0 & 1 & 0
\end{array} 
$$
\caption{Beispiel einer Co-occurence-Matrix}
\label{fig:COOC-MAT}
\end{figure}

Aus dieser Matrix soll für jedes Wort ein Vektor aufgebaut werden, der die Lage des Wortes in einem $n$-dimensionalen Raum beschreibt. Dieser Vektor kann später für die Berechnung vom Abstand zwischen Wörtern verwendet werden, was in den späteren Schritten als Maße für ,,Gleichheit`` von Wörtern verwendet werden kann.

\paragraph{}
Ein Beispiel für auf Cluster basierte Features wäre Brown-Algorithmus\cite{sun2011semi}. Dieses Algorithmus funktioniert auf folgende Art und Weise:
\begin{enumerate}
\item Zuerst wird jedem Wort eine eigene Klasse zugeordnet
\item Die Clusters werden paarweise iterativ zusammengefügt.
\item Dabei werden die Zwischenschritte der Aufbau von Clusters gespeichert, so dass man am Ende alle Schritte nachverfolgen kann, und ein binäres Baum aufbauen könnte, wo jeder Endknoten ein Wort darstellt.
\end{enumerate}

Eine graphische Darstellung des Brown-Algorithmus findet man auf der Abbildung \ref{fig:BROWN-CLUSTER}.

\begin{figure}[ht]
\setbox0\vbox{\small}
\begin{tikzpicture}[>=Stealth]
\graph[binary tree layout]{
  ROOT -> {   
    C1 -> { 
      C12 -> { 
        C123 -> { Wort1, Wort2 },
        Wort9
      }, 
    C2 -> { Wort3, Wort4 }
    },
      C3 -> {
        C31 -> { Wort5, Wort6 },
        C32 -> { Wort7, Wort8 }
      }
  }
};
\end{tikzpicture}
\caption{Brown-Algorithmus}
\label{fig:BROWN-CLUSTER}
\end{figure}

\paragraph{}
Ein sehr interessanter Ansatz fürs ,,Word Embedding`` wird in der Arbeit von Xin\cite{rong2014word2vec} ausführlich erklärt. Wie in ,,Distributional Features``-Modellen wird für die Wörter ein Merkmalsvektor berechnet. Dabei bestimmt der geometrische Abstand zwischen zwei Punkten im Feature-Raum die Ähnlichkeit von Wörtern, zusätzlich dazu kann auch die Ausrichtung des Vektors berücksichtigt werden. Die entsprechende Vektoren werden mit Hilfe von einem neuralen Netz berechnet, das auf zwei verschiedene Art und Weisen implementiert werden kann: \textit{CBOW} und \textit{Skip-gram}\cite{wang2014introduction}:

Die erste Methode - CBOW (Continuous Bag Of Words\cite{garcia2014word}) - gibt dem Entwickler die Möglichkeit, die Features für ein Wort anhand von Wörtern in seiner Umgebung zu berechnen, es wird also versucht, die Wahrscheinlichkeit $P(Wort|Kontext)$ zu maximieren.

Der zweite Ansatz - Skip-gramm - wird in der Arbeit von Mikolov\cite{mikolov2013distributed} beschrieben. Dieser Einsatz lässt sich als eine Inversion von CBOW  definieren - es wird hier versucht, die Features für Umgebungswörter zu berechnen, und die Wahrscheinlichkeit $P(Kontext|Wort)$ zu maximieren.

Für die Analyse von großen Textmengen soll CBOW bevorzugt werden, da dieses Algorithmus bessere Geschwindigkeit aufzeigt\cite{wang2014introduction}. 

\paragraph{}
Es darf allerdings nicht vergessen werden, dass bei größeren Datenmengen noch weitere Anpassungen an den Algorithmen durchgeführt werden müssen. Sahlgren\cite{sahlgren2006word} spricht unter anderem das Problem von ,,Lücken`` in den Co-occurence-Matrix an: es gibt viele Wortpaare, die im Korpus niemals auftreten, und so beinhalten entsprechende Co-occurence-Vektoren Nullen, was auch auf der Abbildung \ref{fig:COOC-MAT} zu sehen ist. Auch bei der Verwendung von anderen Algorithmen muss die Anzahl von Dimensionen gegebenfalls reduziert werden.

Sobald die Eigenschaften bestimmt wurden, können mit ihrer Hilfe die am Anfang des Kapitels erwähnte Algoreithmen die Entitäten zu extrahieren versuchen.

\subsection{Conditional Random Field} \label{subsec:crftheory}
%Was ist CRF? Wie funktioniert dieser Einsatz? Wo wird er verwendet?
\paragraph{}
CRFs wurden vom Charles Sutton und Andrew McCallum in ihrer Arbeit\cite{Charles/Andrew:10} beschrieben. Die Grundidee dieses Algorithmus besteht darin, die Abhängigkeit einer Sequenz von Ausgängen (Entitätsklassen) $y$ von einer gegebenen Sequenz von Wörtern $x$ mithilfe eines Graph zu modellieren. Wichtig dabei ist, dass CRF im Gegenteil zu ME (Sektion \ref{sec:MEGRUND}) oder zu SVM (Sektion \ref{sec:SVNGRUND}) eine \textbf{Sequenz} von Wörtern und nicht einzelne Wörter klassifiziert\cite{rossler2007korpus}.

Der Aufbau eines CRF-Graphs ist auf der Abbildung \ref{fig:CRF-Modell} dargestellt. Die weiß markierte Knoten $y$ beschreiben in einem CRF-Modell mögliche Ausgänge des Klassifikators, bei einem NER-Klassifikator sind das alle mögliche Entitätsklassen. Features werden durch grau markierte Knoten $x$ dargestellt. Die Verbindungen zwischen Knoten $x$ und $y$ beschreiben dabei Abhängigkeiten zwischen Features und Entitätsklassen. Jeder Übergang $x \longrightarrow y$ besitzt ein Gewicht, das die Wahrscheinlichkeit, dass Wort mit der Eigenschaft $x$ eine Entität der Klasse $y$ ist, beschreibt.

Eine entscheidende Eigenschaft von CRF ist, dass bei der Berechnung von Übergängen die gesamte Zustandskette berücksichtigt wird, und nicht nur aktueller und vorheriger Knoten\cite{rossler2007korpus}, was bedeutet, dass auch zukünftige Zustände den Einfluss auf die Klassifizierung vom aktuellen Wort haben, was das sogenannte Label-Bias-Problem\cite{lafferty2001conditional} löst.

\begin{figure}[ht]
\setbox0\vbox{\small}
\includegraphics[width=0.7\textwidth]{Bilder/crf-modell-charles-andrew}
\caption{CRF-Modell (die Abbildung ist aus der Arbeit von Charles Sutton \cite{Charles/Andrew:10} entnommen)}
\label{fig:CRF-Modell}
\end{figure}

Das CRF-Algorithmus zeigt folgende Vor- und Nachteile auf:
\begin{itemize}
\item Vorteile:
\begin{itemize}
\item Schnelles Trainieren von Modellen\cite{sha2003shallow}, im Vergleich zu SVM (Siehe die Sektion \ref{sec:SVNGRUND}).
\item Die Abhängigkeiten zwischen Features können berücksichtigt werden.
\item Label-Bias-Problem wird im CRF-Algorithmus gelöst.
\item Im Vergleich zu ME (Siehe Sektion \ref{sec:MEGRUND} können nicht nur binäre, sondern beliebige Features verwendet werden.
\end{itemize}
\item Nachteile:
\begin{itemize}
\item Die Implementierung von einem auf CRF basierten NER-Extraktor kann komplizierter als die von anderen Algorithmen sein.
\item Die Präzision von einem CRF-Modell soll geringer als die vom SVM sein, was während der Evaluierung (Sektion \ref{sec:sysevalsec}) bestätigt wird.
\end{itemize}
\end{itemize}

\subsection{Maximum Entropy based NER} \label{sec:MEGRUND}
%Was ist ME NER? Wo wird er verwendet? Welche Vor- und Nachteile hat dieser Einsatz im Vergleich zum CRF?
\paragraph{}
Dieses Framework wurde in der Arbeit von Andrew Borthwick\cite{borthwick1999maximum} vorgestellt. ME-Framework operiert grundlegend mit folgenden Begriffen:
\begin{enumerate}
\item \textit{Zukunft} (In \cite{borthwick1999maximum} ,,Futures`` genannt) - Eine Menge von möglichen Ausgaben des Algorithmus. Im Fall von Extraktion von Entitäten bezeichnen die ,,Futures`` alle mögliche Entitätsklassen.
\item \textit{Historie} - diese Bezeichnung könnte verwirrend wirken, da dieser Begriff eigentlich die Umgebung eines Wortes beschreibt (so eine Art vom Fenster, wo das Wort, das analysiert werden muss, in der Mitte steht).
\item \textit{Features} - wie bei anderen Algorithmen beschreibt dieser Begriff ein Vektor von Eigenschaften eines Wortes.
\end{enumerate}

Wie CRF, ist ME ein graphenbasiertes Modell, das Abhängigkeiten zwischen Features und Entitätsklassen modelliert, mit folgenden Unterschieden:
\begin{itemize}
\item Bei der Berechnung von Übergängen wird nur aktueller und vorheriger Zustände berücksichtigt, und nicht die ganze Kette, wie es bei CRF der Fall ist\cite{lafferty2001conditional}\cite{rossler2007korpus}.
\item Es werden nur binäre Features\cite{ratnaparkhi1998maximum}\cite{borthwick1999maximum} eingesetzt.
\end{itemize}

Aus diesen beiden Merkmalen folgen beide Nachteile dieses Modells:
\begin{enumerate}
\item Im Vergleich zu CRF\cite{rossler2007korpus} geringere Genauigkeit.
\item Label-Bias-Problem wird bei dem Ansatz dieses Algorithmus nicht gelöst\cite{lafferty2001conditional}.
\end{enumerate}
Der Vorteil dieses Modells ist die Geschwindigkeit des Algorithmus, die größer als bei der Verwendung von CRF sein soll\cite{Jenny/etal:07}. Aus dem Grund, dass bei der Extraktion von Entitäten aus Suchergebnissen die Geschwindigkeit der Suche wichtig ist, wird die dieses Algorithmus implementierende API\footnote{\url{https://opennlp.apache.org/} (Zuletzt abgerufen am 02. November)} im Rahmen dieser Arbeit mit anderen Algorithmen zusammen verwendet.

\subsection{SVM} \label{sec:SVNGRUND}
\paragraph{}
Support Vector Machines (oder Stützvektormaschine) ist eine Methode zur Klassifizierung von Daten, deren Grundidee\cite{meyer2014support} auf der Abbildung \ref{fig:SVM-INTRO} graphisch dargestellt ist. 

\begin{figure}
\centering
\includegraphics[width=\textwidth,angle=90]{Bilder/svm-intro.png}
\caption{''Die Klassifizierung mithilfe von Stützvektoren''}
\label{fig:SVM-INTRO}
\end{figure}

Jeder Punkt auf diesem Diagramm entspricht einem Objekt, das klassifiziert werden muss, im Fall von NER stellen die Punkten die Wörter, für die eine entsprechende Entitätsklasse berechnet werden muss, dar. Die Position jedes Punktes in einem $n$-dimensionalen Raum entspricht dem Merkmalsvektor des Wortes. Um die Abbildung \ref{fig:SVM-INTRO} anschaulich gestalten zu können, wird angenommen, dass jedes Wort nur zwei Features besitzt - beide Merkmale werden durch Position des Punktes auf $y$ und $x$ Achsen modelliert.

Das Algorithmus muss eine Trennebene (auf dem Beispieldiagramm ist das die schwarze Gerade im Mitte des Bildes) finden, die die Punkte auf dem Diagramm in zwei Klassen (rote und weiße Hälfte des Diagramms) teilt. Dabei soll diese Trennebene so ausgewählt werden, dass der Abstand zwischen beiden getrennten Klassen möglichst groß bleibt. Dazu werden für jede Klasse sogenannte Stützvektoren (Support Vectors) ausgewählt, die am nächsten zu der Hyberebene liegen sollen. Auf dem Diagramm \ref{fig:SVM-INTRO} sind die Stützvektoren weiß angekreuzt.

Der Abstand zu der Trennebene beschreibt dabei die Wahrscheinlichkeit, dass ein Wort zu der ausgewählten Klasse gehört - je weiter von der Trennebene das Wort liegt, desto größer ist die Wahrscheinlichkeit, dass das Wort korrekt klassifiziert wurde.

Damit so eine Hyperebene aufgebaut werden könnte, ist es wichtig, dass die Datenpunkte linear trennbar wären. Für ein zweidimensionales Raum bedeute das, dass beide Klassen durch eine Gerade trennbar sein müssen. Wenn die zu klassifizierende Punkte allerdings so angeordnet sind, dass es nicht möglich ist, die linear zu trennen, wird die Klassifizierung falsch durchgeführt, was auf der Abbildung \ref{fig:SVM-NONLINEAR-ISSUE} zu sehen ist.

\begin{figure}
\centering
\includegraphics[width=\textwidth,angle=90]{Bilder/svm-nonlinear-issue.png}
\caption{''SVM Algorithmus angewandt auf nicht linear trennbare Daten''}
\label{fig:SVM-NONLINEAR-ISSUE}
\end{figure}

Auf der Abbildung \ref{fig:SVM-NONLINEAR-ISSUE} sind die Datenpunkte so angeordnet, dass die Wörter, die zu erster Klasse zugehören, in der Mitte stehen, umkreist von den Wörtern, die zu zweiter Klasse zugehören. Da SVM aber nur eine lineare Trennfläche aufbauen kann, werden die Klassen so geteilt, dass die erste Klasse unter der Trennlinie und die zweite oben der Linie liegt, was einer richtigen Unterteilung von Klassen nicht entspricht.

Allerdings sind meiste Daten, mit denen man in der realen Welt arbeiten muss, nicht linear trennbar, und damit kann das SVM-Algorithmus nicht in seiner ursprünglichen Form verwendet werden. Allerdings gibt es eine Möglichkeit, mit einer Erweiterung des SVM-Algorithmus auch nicht linear trennbare Daten zu klassifizieren. Dazu muss ursprüngliches Eigenschaftsraum $R^n$ mithilfe von sogenannter Kernfunktion auf ein Raum höherer Dimension $R^{n+1}$ abgebildet werden, und zwar so, dass die Datenpunkte im neuen Raum linear trennbar wären\cite{Hearst:98}. Auf diese Art und Weise kann SVM-Algorithmus auf beliebige Merkmalsräume angewendet werden, wenn eine passende Kernfunktion ausgewählt wurde\cite{hsu2003practical}.

\begin{figure}
\centering
\includegraphics[width=\textwidth,angle=90]{Bilder/svm-nonlinear-rbf.png}
\caption{''SVM-Algorithmus mit einem RBF-Kern angewandt auf nicht linear trennbare Daten''}
\label{fig:SVM-NONLINEAR-ISSUE-FIXED}
\end{figure}

Die Abbildung \ref{fig:SVM-NONLINEAR-ISSUE-FIXED} zeigt, wie sich ursprünglich nicht-linear-trennbare Daten durch Einsatz einer Kernfunktion korrekt trennen lassen. Die Datenpunkte werden genau so wie auf der Abbildung \ref{fig:SVM-NONLINEAR-ISSUE} angeordnet, die erste Klasse in der Mitte, umkreist von der zweiten Klasse, diesmal werden beide Klassen aber korrekt getrennt, durch eine ovale Trennfläche, die in einem Raum höherer Dimension linear ist.

%Und wie kann man SVMs in NEr einsetzen?
Der Einsatz von Stützvektoren für Extraktion von Entitäten ist in der Arbeit von Kazama\cite{kazama2002tuning} beschrieben. Obwohl es in dieser Arbeit um Extraktion von Entitäten aus englischsprachigen medizinischen Texten geht, kann dasselbe Prinzip auch auf deutsche Texte angewendet werden, wenn man das richtige Korpus für Training zur Verfügung stellt. Um SVMs für Extraktion von Entitäten einsetzen zu können, muss noch eine Erweiterung zum ursprünglichen Algorithmus gemacht werden, zusätzlich zum Kerneleinsatz - Stützvektoren können generell zwischen zwei Klassen von Objekten unterscheiden (ein Stützvektoreinsatz kann sagen, ob das Objekt der Klasse $C_a$ oder der Klasse $C_b$ gehört), und bei Entitäten gibt es mehr als zwei Klassen. Um diese Beschränkung umzugehen, wird SVM-Algorithmus auf eine der folgenden Arten und Weisen erweitert:

\begin{itemize}
\item Für jede mögliche Entitätsklasse wird ein SVM aufgebaut, der entscheidet, ob das Token der Klasse $C_a$ oder dem Rest der Klassen gehört.
\item Für jede Paar $(C_a, C_b)$ von möglichen Entitätstypen wird ein SVM erzeugt, der feststellen muss, ob das Wort den Typ $C_a$ oder $C_b$ hat. Die Klasse, die von der Mehrheit von SVMs gewählt wurde, gewinnt.
\end{itemize}

Vorteile des SVM-Ansatzes anderen Methoden gegenüber\cite{joachims1998text}:
\begin{itemize}
\item SVMs eignen sich gut für die Bearbeitung von langen Featuresvektoren, was bei Klassifizierung von Wörtern ein großer Vorteil ist.
\item Dieses Algorithmus funktioniert mehr oder weniger gleich auf allen Domänen von Texten.
\item Es ist üblicherweise kein Tuning von Traininsparametern notwendig.
\end{itemize}

Nachteile von SVMs anderen Algorithmen gegenüber:
\begin{itemize}
\item Im Vegrleich zu ME oder CRF langsamer Trainingsvorgang - bei größeren Trainigskorpora kann die Erstellung eines Modells bis auf mehrere Tagen in Anspruch nehmen.
\item Anforderungen zur Größe des Arbeitsspeichers sind üblich auch höher als bei anderen Algorithmen, was während der Implementierung (siehe Sektion \ref{subsec:TRMODELLS}) bestätigt wird.
\end{itemize}

\section{Trainingskorpora} \label{sec:trcorpora}
Jedes im Rahmen dieser Arbeit betrachtetes Algorithmus braucht ein vortrainiertes Modell, um Entitäten aus einem Text extrahieren zu können. Manchmal können zwar schon fertige Modelle verwendet werden, die schon vorher trainiert wurden, aber wenn es kein solches Modell verfügbar ist, muss der Entwickler ein eigenes trainieren.

Um ein Modell trainieren zu können, braucht man ein Textkorpus. Korpus ist eine Sammlung von Texten, die entweder von Linguisten (manuell annotierete Korpora) oder automatisch (maschinell erzeugte Korpora) mit Annotationen versehen wurden. Die Annotationen können im Prinzip beliebig sein (z.B. POS-Tags sind auch Annotationen), aber im Rahmen der Erkennung von Entitäten braucht man ein Korpus, wo die Wörter, die Entitäten darstellen, markiert sind \cite{naf2006korpuslinguistik}.

Ein Beispiel für ein von Linguisten vorbereitetes Korpus wäre auf Zeitungen und Zeitschriften aufgebautes CoNLL \cite{tjong2003introduction}. Die Daten, die im Rahmen der verlinkten Arbeit gesammelt wurden, haben folgende Vorteile:
\begin{itemize}
\item Da dieses Korpus auf Zeitschriften aufgebaut ist, weisen die Daten relativ gute Qualität auf, vor allem im Bezug auf Schreibfehler.
\item Das in diesem Korpus verwendetes Datenformat ist ein Industriestandard und ist deswegen bereits in vielen Bibliotheken für Analyse von natürlichsprachlichen Texten implementiert. 
\item Ein Korpus für deutsche Sprache ist auch verfügbar, was die Daten nützlich für diese Masterarbeit machen könnte.
\end{itemize}
Leider konnten diese Daten im Rahmen dieser Arbeit nicht verwendet werden, da die öffentlich nicht verfügbar sind, und nur gegen Aufpreis zur Verfügung gestellt werden können. In der Sektion \ref{sec:extraktimpl} des Kapitels ,,Implementierung`` wird es deswegen auf Alternativlösungen zugegriffen.

Eine Alternative zu den auf Basis von Zeitungen aufgebauten Korpora wären die webbasierte Korpora\cite{liu2006web}. Im Vergleich zu den auf Zeitschriften basierten Korpora, weisen die aus Web extrahierte Daten größere Varianz auf, und wären deswegen für generische Domäne nützlicher sein können. Es muss aber gleichzeitig beachtet werden, dass die aus Web extrahierte Daten größere Fehlerraten aufweisen können, besonders im Bezug auf Grammatik. Ein Beispiel für ein webbasiertes Korpus wäre DeWaC\cite{baroni2009wacky}.

Zusätzlich zu manuell erstellten Korpora könnte ein Korpus auch automatisch erstellt werden, wie es z.B. für POS-Annotationen in der Arbeit von Marcus\cite{marcus1993building} gemacht wurde. In der Sektion \ref{subsec:decor} des Kapitels ,,Implementierung`` wird die Erstellung eines Korpus für Entitätsextraktion auf Basis von aus Wikipedia extrahierten Daten näher beschrieben.

Beim Training eines NER-Modells auf einem Korpus muss außerdem folgendes im Kauf genommen werden: bei vielen frei verfügbaren Korpora sind die Klassen von Entitäten nicht explizit eingegeben - die Entitäten sind in vielen Datensätzen zwar annotiert, aber weitere Angaben zum Typ der Entität werden nicht gemacht. Deswegen kann ein NER-Modell den Unterschied zwischen verschiedenen Klassen von Entitäten öfters nicht lernen, was dazu führt, dass die entsprechende Klassen nachdem die Entitäten aus einem Text extrahiert wurden aus einer Datenbank geholt werden müssen, falls die Daten vorhanden sind.

\section{Wissendatenbanke} \label{sec:wiss}
%Was sind Wissendatenbanke? Wieso brauchen wir die in unserer Arbeit? Wie sucht man nach Entitäten in einer Wissendatenbank? Kurze Einleitung in SPARQL+RDF.
\paragraph{}
Um die extrahierte Entitäten mit Ontologien anzureichern, braucht man selbstverständlich eine Datenbank, wo alle Informationen zu den Entitäten gespeichert werden. Solche Datenbanken nennt man ,,Wissendatenbanken`` - eine Datenbank, wo ,,Wissen`` über ein bestimmtes Domain gespeichert werden. Es stellt sich die Frage, warum die ,,Wissendatenbanken`` explizit von den herkömmlichen relationalen Datenbanken getrennt behandelt werden müssen. Welche Besonderheiten weisen Wissendatenbanke auf, die Anwendung von relationalem Datenmodell schwer (beziehungsweise unmöglich) machen?

Aus den verschiedenen Werken über relationale Algebra\cite{meier2013relationale} ist es bekannt, dass relationale Datenbanken ,,fest`` strukturiert sind - die Daten sollen einer vordefinierten Schema (Beschreibung der Datenstruktur) folgen. Jede Änderung von Datenstruktur (das Hinzufügen von neuen Feldern z.B.) führt zu einer Schemaänderung. Es ist allerdings sehr schwer, solche ,,feste`` Schema für Entitäten zu definieren, da:
\begin{itemize}
\item Verschiedene Entitäten können verschiedene Menge von Eigenschaften besitzen - ein geographisches Objekt kann kein Geburtstag haben, eine Person allerdings schon.
\item Sowohl die Klassen als auch die Eigenschaften können sich mit der Zeit ändern - neue Klassen können hinzugefügt und alte Eigenschaften gelöscht werden.
\item Die Entitäten können mithilfe von Links miteinander verbunden sein, und diese Links können zyklisch sein, was auf der Abbildung \ref{fig:cyclent} dargestellt ist. Es ist zwar möglich, zyklische Datenstrukturen mithilfe von relationalen Datenbanken abzubilden versuchen, die Aufbau solcher Datenbank wäre aber zu komplex.
\end{itemize}

\tikzset{main node/.style={circle,fill=blue!20,draw,minimum size=1cm,inner sep=0pt}}
\begin{figure}[ht]
\setbox0\vbox{\small}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
\node[main node] (Deutschland) {Deutschland};                      
\node[main node] (Merkel) [above = 3cm of Deutschland] {Angela Merkel};     

\path (Deutschland) edge [bend left] node {Bundeskanzler} (Merkel)
      (Merkel) edge node {Geboren in} (Deutschland);                
\end{tikzpicture}
\caption{Beispiel von zyklischen Verlinkungen zwischen Entitäten}
\label{fig:cyclent}
\end{figure}
Aber wie können dann ,,Wissen`` über die Entitäten gespeichert werden? Welche Datenbanken und Datenstrukturen können hierfür verwendet werden? Gibt es schon bereits eine Lösung oder muss ein eigenes Konzept entwickelt werden?

Für die Darstellung von Wissen wurde von dem W3-Consortium\cite{klyne2006resource} eine spezielle auf XML basierende Datenstruktur RDF (Resource Description Framework) entworfen, die Speicherung von vage strukturierten Daten ermöglicht, was für die Speicherung von Informationen über Entitäten am besten passt. Dieses Format basiert sich auf gerichteten Graphen (siehe die Abbildung \ref{fig:graph-rdf}) und umfasst folgende Grundbegriffe:
\begin{itemize}
\item Subjekt - die Entität, deren Eigenschaft beschrieben werden muss (z.B. eine Person).
\item Prädikat sagt, welche Eigenschaft der Entität beschrieben wird (z.B. Alter einer Person kann ein Prädikat sein).
\item Objekt - das Wert der Eigenschaft. Dabei kann Objekt eine weitere Entität sein, und eigene Eigenschaften besitzen. 
\end{itemize}
Ein Objekt kann also auch die Rolle eines Subjektes spielen und umgekehrt, was die Speicherung von komplexen Strukturen ermöglicht.

\tikzset{main node/.style={circle,fill=blue!20,draw,minimum size=1cm,inner sep=0pt}}
\begin{figure}[ht]
\setbox0\vbox{\small}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
\node[main node] (Subjekt) {Subjekt};                      
\node[main node] (Objekt) [above = 3cm of Subjekt] {Objekt};     

\path (Subjekt) edge [bend left] node {Prädikat} (Objekt);             
\end{tikzpicture}
\caption{Grundidee des RDF-Datenformats}
\label{fig:graph-rdf}
\end{figure}

Als Backend für die Speicherung von diesen Graphen wird eine Erweiterung von XML benutzt. Ein Beispiel für eine Entität im RDF/XML-Format ist in der Auflistung \ref{lst:RDFBEISPIEL} zu sehen.

\lstset{language=XML}
\lstinputlisting[captionpos=b,label={lst:RDFBEISPIEL},caption={Beispiel einer Ontologie im RDF-Format}]{Listings/examplerdf.xml}

Nachdem das Format für die Datenspeicherung definiert wurde, muss auch eine Anfragesprache definiert werden, mit deren Hilfe auf Informationen in einer Wissendatenbank zugegriffen werden kann. Es ist natürlich klar, dass es hierfür kein SQL verwendet werden kann, da es mit Graphen, und nicht mit relationalen Datenbanken gearbeitet wird. Für die Abfrage von RDF-Daten wird SPARQL (SPARQL Protocol And RDF Query Language) - verwendet. Diese Sprache wurde genau wie RDF-Datenformat vom W3-Consortium entworfen, und ist in der Arbeit von Quilitz\cite{quilitz2008querying} näher beschrieben.

Am besten kann diese Sprache anhand des auf der Auflistung \ref{lst:SPARQLBEISPIEL} abgebildetes Beispiels erklärt werden.
\lstset{language=SPARQL}
\lstinputlisting[captionpos=b,label={lst:SPARQLBEISPIEL},caption={Beispiel einer SPARQL-Anfrage}]{Listings/sparqlexample.sql}

Wie es schon beschrieben wurde, arbeitet SPARQL mit RDF-Graphen. Obwohl diese Abfrage einer SQL-Anfrage ähnlich ist, darf man beide Sprachen nicht verwechseln, da die mit ganz verschiedenen Datentypen arbeiten. 

Eine typische SPARQL-Anfrage ist wie folgt aufgebaut:
\begin{itemize}
\item Der erste Teil der Anfrage - das Schlüsselwort ,,SELECT`` beschreibt, welche Eigenschaften (oder Prädikate in der Terminologie von RDF) einer Entität aus der Datenbank gelesen werden müssen. In dem Beispiel werden die Prädikaten ,,Fläche`` und ,,Bundesland`` benötigt.
\item Der zweite Teil der Anfrage beschreibt, welche Entitäten ausgewählt werden müssen. Auf den ersten Blick sieht die Konstruktion im WHERE-Teil der Anfrage ziemlich kompliziert aus, aber wenn man sich die Abbildung \ref{fig:graph-rdf} anguckt, dann wird es klar, dass es nur so eine Art von einem Muster angegeben wird, das beschreibt, wie Subjekt, Prädikat und Objekt zusammen aussehen müssen, um in der Liste von Ergebnissen aufzutauchen. Die Anfrage im Beispiel \ref{lst:SPARQLBEISPIEL} sagt, dass alle Entitäten, deren Prädikat ,,rdfs:label`` das Wert ,,Duisburg`` hat, und die  außerdem die Eigenschaften ,,dbo:areaTotal`` und ,,dbo:federalState`` besitzen (deren Werte beliebig sein dürfen), aus der Datenbank ausgelesen werden müssen.
\end{itemize}

Optional können die Ergebnisse auch sortiert werden, mithilfe von der ,,ORDER BY``-Operation. Die Ergebnisse lassen sich außerdem mithilfe von regulären Ausdrücken präziser rausfiltern,

Als Antwort auf eine SPARQL-Anfrage wird üblicherweise ein RDF-Graph geliefert. Ein Beispiel für die Antwort auf vorher eingegebene Anfrage findet man in der Auflistung \ref{lst:SPARQLRESULT}.

\lstset{language=XML}
\lstinputlisting[captionpos=b,label={lst:SPARQLRESULT},caption={Beispiel der Antwort auf eine SPARQL-Anfrage}]{Listings/sparqlresult.xml}

\section{Zwischenfazit}
Nachdem die Grundlagen dieser Arbeit erklärt wurden, können schon folgende Bemerkungen für den Implementierungsschritt erläutert werden:
\begin{itemize}
\item Training eines Modells stellt eine getrennte Aufgabe dar, und wird deswegen gesondert implementiert werden müssen.
\item Da nur wenige Korpora frei verfügbar sind, wird es unter anderem auf automatisch erzeugte Korpora zugegriffen, was die Qualität der Extration beeinflussen wird.
\item Für den Vergleich zwischen verschiedenen Algorithmen sollen verschiedene Backends implementiert werden, die über eine universale Schnittstelle erreichbar werden müssen.
\item Es muss für die Verlinkung von Entitäten nachgedacht werden, wo genau die Daten über die Entitäten gefunden werden können.
\end{itemize}
